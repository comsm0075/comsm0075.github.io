

<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<font face="verdana, Sans-serif">
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="GENERATOR" content="Mozilla/4.51 [en] (X11; I; OSF1 V4.0 alpha) [Netscape]">
   <title>COMSM0075
</title>
</head>

<body  bgcolor=white link="slategrey" vlink="darkslategrey" alink="darkslategrey">

  <table>
    <tbody>
      <tr>
	<td bgcolor="slategrey" width="30">&nbsp;</td>
	
	<td bgcolor="darkslategrey" width="1">&nbsp;</td>
	<td bgcolor=white width="10">&nbsp;</td>
	
	<td width="15"> 
	<td>
	  
	  <h1 style="color: darkslategrey">COMSM0075</h1><p>
	    
	    <h2 style="color: darkslategrey">Week 1 - CH - information theory</h2>
	    <ul>
	      <li><b>lecture 1</b>: introduction to the unit, motivation, definition of entropy and some of its properties. This corresponds to, roughly, the first four pages of "notes1".</li>
	      <li><b>notes1</b> lecture notes for information theory <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/notes1_information.pdf">pdf</a></li>
	      <li><b>lecture 2</b>: more on information theory, the source coding theorem, joint and conditional entropy, mutual information and the information processing inequality.</li>
	      	      <li>old online lecture motivating Shannon's entropy <a href="https://www.youtube.com/watch?v=aD7PCVgdPzU">youtube</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/slides1.pdf">slides (pdf)</a></li>
	      <li>old online lecture: Shannon's entropy <a href="https://www.youtube.com/watch?v=aeP3JsceyxY">youtube</a> / <a href="https://github.com/comsm0075/2021_22/raw/master/1_information_theory/slides2.pdf">slides (pdf)</a></li>
<li>old online lecture: joint and conditional entropy <a href="https://youtu.be/4N4nKGWun40">youtube</a> / <a href="https://github.com/comsm0075/2021_22/raw/master/1_information_theory/slides3.pdf">slides (pdf)</a></li>
<li>old online lecture: mutual information <a href="https://youtu.be/vYPR5_docCE">youtube</a> / <a href="https://github.com/comsm0075/2021_22/raw/master/1_information_theory/slides4.pdf">slides (pdf)</a></li>
	      <li>old online lecture: the data processing inequality <a href="https://youtu.be/6s1JpK0PbnA">youtube</a> / <a href="https://github.com/comsm0075/2021_22/raw/master/1_information_theory/slides5.pdf">slides (pdf)</a></li>
		 <li><b>worksheet 1</b> (based on week 1). <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/worksheet1.pdf">pdf</a></li>
	      
</ul>
	    <h2 style="color: darkslategrey">Week 2 - CH - information theory in the brain</h2>
	    <ul>
	    <li><b>lecture 3</b>: recap of mutual information, information and the brain.</b>
	    <li><b>lecture 4</b>: this was a problem class but you were also asked to watch the differential entropy lecture and read the notes.</b>
		 <li>lecture notes for information in the brain <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/notes2_in_the_brain.pdf">pdf</a></li>
		 <li>lecture notes for differential entropy <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/notes3_differential.pdf">pdf</a></li>
		 <li>lecture 6 information in the brain <a href="https://youtu.be/g8V2LMfFAwc">youtube</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/slides6.pdf">slides (pdf)</a></li>
		 <li>lecture 7 differential entropy examples <a href="https://youtu.be/GKZ2fJrm_D4">youtube</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/slides7.pdf">slides (pdf)</a></li>
		 <li>lecture 8 differential entropy and Shannon's entropy <a href="https://youtu.be/r3QWJgm_zDI">youtube</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/slides8.pdf">slides (pdf)</a></li>
		 <li>lecture 9 mutual information for continuous probabilities <a href="https://youtu.be/fM7RFNwtJuI">youtube</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/1_information_theory/slides9.pdf">slides (pdf)</a></li>
	    </ul>
	    <h2 style="color: darkslategrey">Week 3 - CH - the Bayesian Brain / Kalman filters</h2>
<ul>
	    <li><b>lecture 5</b>: the Bayesian brain.</b>
	    <li><b>lecture 6</b>: more on the Bayesian brain, start of Kalman filtering.</b>
<li> for a gentle revision of Bayes's rule see: <a href="https://github.com/coms10011/2020_21/blob/master/03_bayes/3_bayes.pdf">coms10011 note on Bayes</a></li>
<li>lecture notes revising Bayes's rule <a href="https://github.com/comsm0075/2022_23/blob/master/2_Bayesian_brain/notes1a_Bayesian.pdf">pdf</a></li>
  <li>lecture notes on Bayesian fusion in the brain <a href="https://github.com/comsm0075/2022_23/blob/master/2_Bayesian_brain/notes1b_Bayesian.pdf">pdf</a></li>
<li>lecture notes on the Kalman filter <a href="https://github.com/comsm0075/2022_23/blob/master/2_Bayesian_brain/notes2_Kalman.pdf">pdf</a></li>
<li> note that infomax wasn't covered this year</li>
</ul>
	    
	    <h2 style="color: darkslategrey">Week 4 - RPC - Neural circuits and learning</h2>
    <ul>
      <li>L1: Neural circuits and learning: introduction <a href="https://web.microsoftstream.com/video/c1bf26ba-0361-45ca-a5e1-47ea1fb538c7" target="_blank">Stream</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/RPC_lectures/l1_ncl_intro_learning_paradigms.pdf" target="_blank">slides (pdf)</a></li>
      <li>L2: Supervised learning and backprop <a href="https://web.microsoftstream.com/video/5410988d-e38e-49e4-ae57-51877987fdfa" target="_blank">Stream</a> / <a href="https://github.com/comsm0075/2022_23/raw/master/RPC_lectures/l2_ncl_suplearn_backprop.pdf" target="_blank">slides (pdf)</a> </li> 

      <li>Guest lecture (Charles Kind): Introduction to neuromorphic computing <a href="https://github.com/comsm0075/2022_23/raw/master/guest_lectures/NeuromporphicComputing.pdf" target="_blank">slides (pdf)</a> /  <a href="https://github.com/comsm0075/2022_23/raw/master/guest_lectures/div1.mp4" target="_blank">video</a></li>
    </ul> 

	    <h2 style="color: darkslategrey">Week 5 - RPC - Neural circuits and learning</h2>

	        <ul>
      <li><u>Lab 1</u>: Supervised learning and backprop (<a href="https://github.com/comsm0075/2022_23/blob/master/RPC_labs/lab1/IPB_labQ1.ipynb" target="_blank">link</a>; solutions) </li>
    </ul> 

	    <h2 style="color: darkslategrey">Week 6 - Reading week</h2>

	    <h2 style="color: darkslategrey">Week 7 - RPC - Neural circuits and learning</h2>

	    <h2 style="color: black">Week 8 - RPC - Neural circuits and learning</h2>

	    <h2 style="color: darkslategrey">Week 9 - 11 Coursework weeks</h2>    
	    
</body>
</html>
