

<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<font face="verdana, Sans-serif">
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="GENERATOR" content="Mozilla/4.51 [en] (X11; I; OSF1 V4.0 alpha) [Netscape]">
   <title>COMSM0075
</title>
</head>

<body  bgcolor=white link="slategrey" vlink="darkslategrey" alink="darkslategrey">

  <table>
    <tbody>
      <tr>
	<td bgcolor="slategrey" width="30">&nbsp;</td>
	
	<td bgcolor="darkslategrey" width="1">&nbsp;</td>
	<td bgcolor=white width="10">&nbsp;</td>
	
	<td width="15"> 
	<td>
	  
	  <h1 style="color: darkslategrey">COMSM0075</h1><p>
	    
	    <h2 style="color: black">Week 1 - CH - information theory</h2>
	       <ul>
		 <li>lecture notes for information theory <a href="https://github.com/comsm0075/2020_21/raw/master/1_information_theory/notes1_information.pdf">pdf</a></li>
		 <li>lecture 1 motivating Shannon's entropy <a href="https://www.youtube.com/watch?v=aD7PCVgdPzU">youtube</a> / <a href="https://github.com/comsm0075/2020_21/raw/master/1_information_theory/slides1.pdf">slides (pdf)</a></li>
<li>lecture 2 Shannon's entropy <a href="https://www.youtube.com/watch?v=aeP3JsceyxY">youtube</a> / <a href="https://github.com/comsm0075/2020_21/raw/master/1_information_theory/slides2.pdf">slides (pdf)</a></li>
<li>lecture 3 joint and conditional entropy <a href="https://youtu.be/4N4nKGWun40">youtube</a> / <a href="https://github.com/comsm0075/2020_21/raw/master/1_information_theory/slides3.pdf">slides (pdf)</a></li>
<li>lecture 4 mutual information <a href="https://youtu.be/vYPR5_docCE">youtube</a> / <a href="https://github.com/comsm0075/2020_21/raw/master/1_information_theory/slides4.pdf">slides (pdf)</a></li>
		 <li>lecture 5 the data processing inequality <a href="https://youtu.be/6s1JpK0PbnA">youtube</a> / <a href="https://github.com/comsm0075/2020_21/raw/master/1_information_theory/slides5.pdf">slides (pdf)</a></li>
	       </ul>

	    <h2 style="color: darkslategrey">Week 2 - CH - information theory in the brain</h2>
	    <h2 style="color: darkslategrey">Week 3 - CH - the Bayesian Brain / Kalman filters</h2>


	    <h2 style="color: darkslategrey">Week 4 - RPC - Neural circuits and learning</h2>
    <ul>
      <li>L1: Neural circuits and learning: introduction</li>
      <li>L2: Supervised learning and backprop </li> 
      <li><u>Lab 1</u>: Supervised learning and backprop </li>
    </ul> 

    <h2 style="color: darkslategrey">Week 5 - RPC - Neural circuits and learning</h2>
    <ul>
      <li>L3: Visual system: deep learning? </li>
      <li>L4: Reinforcement learning </li>
      <li><u>Lab 2</u>: Reinforcement learning </li>
    </ul> 

<h2 style="color: darkslategrey">Week 6 - RPC - Neural circuits and learning</h2>
    <ul>
      <li>L5: Unsupervised learning </li>

      <li>L6: Temporal processing and RNNs </li>
    </ul> 


<h2 style="color: darkslategrey">Week 7 - RPC - Neural circuits and learning</h2>
    <ul>
      <li>L7: Gated RNNs, Brain vs Maquina </li> 
    </ul>     
	    
</body>
</html>
